{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae839c5c-b2b2-4c26-bdfc-88134a59fc80",
   "metadata": {},
   "source": [
    "# Project 3: 16S Microbiome Analysis (Crohn's Disease)\n",
    "\n",
    "## Notebook 00: Setup and Data Download\n",
    "\n",
    "### Objective\n",
    "The goal of this notebook is to prepare our environment and download the raw sequencing data for the project.\n",
    "\n",
    "-   **Project:** `PRJNA450540`\n",
    "-   **Scientific Question:** Compare the gut microbiome of Crohn's Disease (CD) patients in three states (Active, Remission) against Healthy Controls.\n",
    "\n",
    "### Workflow\n",
    "1.  **Inspect Metadata:** Load the `SraRunTable.csv` file (our \"map\") to identify the columns needed for analysis (e.g., Run ID, Disease State).\n",
    "2.  **Install Tools:** Install NCBI `sra-tools` to allow programmatic download from the SRA database.\n",
    "3.  **Download Data:** Use `sra-tools` (specifically `fasterq-dump`) to automatically download all 256 FASTQ files based on the Run IDs from our metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c102bc5-6b66-4b9e-9da4-63bd31830957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set a pandas option to display all columns (not just a few)\n",
    "# This is helpful for wide tables like this one.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the path to our metadata file\n",
    "# We use '../' to go UP one level (from 'notebooks/')\n",
    "# and then DOWN into the 'data/' folder.\n",
    "metadata_file = \"../data/SraRunTable.csv\"\n",
    "\n",
    "# Load the CSV into a pandas DataFrame (df)\n",
    "df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Display the first 5 rows to inspect the columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79e94d-8c8d-40b7-8195-17ed6aa90235",
   "metadata": {},
   "source": [
    "### 2. Investigate Metadata Columns\n",
    "\n",
    "`df.head()` showed us the structure. Now, we need to find all the unique categories within the key columns. This will confirm we have the groups we need (Healthy, Active, Remission) and tell us how they are named."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5f11d-47f3-44ef-aa92-df2a9dea4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the unique values in 'host_subject_id'\n",
    "# .unique() gives us a list of every unique entry\n",
    "# We use list() to make it print nicely\n",
    "subject_ids = list(df['host_subject_id'].unique())\n",
    "\n",
    "print(f\"Total unique subjects found: {len(subject_ids)}\")\n",
    "print(\"---------------------------------\")\n",
    "print(subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c85a6-e914-4a31-a226-b036957455ea",
   "metadata": {},
   "source": [
    "### 3. Verify Group Definitions\n",
    "\n",
    "Our hypothesis from `host_subject_id` is:\n",
    "-   `Control_...` = Healthy\n",
    "-   `patient_..._active` = Active CD\n",
    "-   `patient_...` (no suffix) = Remission CD\n",
    "\n",
    "We must verify this. Let's inspect other columns like `gastrointest_disord` and `PGA` to see what labels they contain. This will confirm our groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22f6a5-6339-4b46-a84b-13c3fb0ba27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the unique values in other potential grouping columns\n",
    "print(\"Unique values in 'gastrointest_disord':\")\n",
    "print(list(df['gastrointest_disord'].unique()))\n",
    "print(\"---------------------------------\")\n",
    "print(\"Unique values in 'PGA':\")\n",
    "print(list(df['PGA'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b01e79-c7fe-4744-8bf4-435f47c01adb",
   "metadata": {},
   "source": [
    "### 4. Conclusion from Metadata\n",
    "\n",
    "The investigation is successful. We have confirmed the presence of our three target groups in the metadata.\n",
    "\n",
    "* The **`Run`** column will give us the SRA IDs for download.\n",
    "* The **`PGA`** column will be our primary scientific variable (Healthy/Active/Remission).\n",
    "* The **`host_subject_id`** column will be used to identify unique individuals (important for longitudinal analysis later).\n",
    "\n",
    "Now we can proceed to the main goal of this notebook: downloading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219683bf-8608-4ee3-9a98-5f20da7a4fe5",
   "metadata": {},
   "source": [
    "### 5. Install SRA-Tools\n",
    "\n",
    "To download the 256 files from NCBI SRA, we need the official `sra-tools` toolkit. We will install it into our `qiime2_env` environment using `mamba`.\n",
    "\n",
    "We use `!` to run a shell command (like `mamba`) directly from inside our Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae0a42-cea9-4729-8f08-6c62d4aa5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sra-tools using mamba\n",
    "# -n qiime2_env: ensures it installs in our specific environment\n",
    "# -c bioconda: the channel where sra-tools is located\n",
    "# -y: automatically says 'yes' to the installation prompt\n",
    "\n",
    "!mamba install -n qiime2_env -c bioconda sra-tools -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea0e33-9188-4e5e-bc21-9fd0598db855",
   "metadata": {},
   "source": [
    "### 6. Prepare for Data Download\n",
    "\n",
    "Now that `sra-tools` is installed, we can download the data.\n",
    "\n",
    "**Warning:** We will *not* run the download command directly in the notebook cell. This process will take hours and will likely time out or crash the notebook kernel.\n",
    "\n",
    "**The Professional Workflow:**\n",
    "1.  Extract the list of all 256 `Run` IDs from our `df` DataFrame.\n",
    "2.  Save this list to a simple text file called `sra_run_list.txt` inside the `data` folder.\n",
    "3.  Create a `bash` script (`download_data.sh`) to download the files.\n",
    "4.  We will then run this script from our **Jupyter Terminal** (which is inside our `screen` session) to ensure it runs safely in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33a01d-b4f8-4c86-9e03-7bab033c880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the 'Run' column from our DataFrame as a list\n",
    "run_ids = df['Run'].tolist()\n",
    "\n",
    "# 2. Check how many IDs we have\n",
    "print(f\"Total Run IDs extracted: {len(run_ids)}\")\n",
    "\n",
    "# 3. Define the path for our new text file\n",
    "output_list_file = \"../data/sra_run_list.txt\"\n",
    "\n",
    "# 4. Write this list to the text file\n",
    "# We use '\\n'.join(run_ids) to write one ID per line\n",
    "with open(output_list_file, 'w') as f:\n",
    "    f.write('\\n'.join(run_ids))\n",
    "\n",
    "print(f\"Successfully saved list to: {output_list_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92847cd-f41d-454e-bab5-8ff74c79c082",
   "metadata": {},
   "source": [
    "### 7. Create the Download Script\n",
    "\n",
    "We will use the `%%writefile` \"magic\" command to write the contents of this cell directly to a new file called `download_data.sh` inside our `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4243a-b59b-4fd3-8a05-e722822f6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../data/download_data.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# This script will download SRA files using fasterq-dump\n",
    "\n",
    "# 1. Define the path to our list of IDs\n",
    "ID_LIST_FILE=\"../data/sra_run_list.txt\"\n",
    "\n",
    "# 2. Define the output directory (where FASTQ files will go)\n",
    "OUT_DIR=\"../data/raw_fastq/\"\n",
    "\n",
    "# 3. Create the output directory if it doesn't exist\n",
    "mkdir -p $OUT_DIR\n",
    "\n",
    "# 4. Loop through each ID in the ID_LIST_FILE\n",
    "while read SRR_ID; do\n",
    "    \n",
    "    echo \"----------------------------------------\"\n",
    "    echo \"Starting download for: $SRR_ID\"\n",
    "    \n",
    "    # Run fasterq-dump\n",
    "    # -e 8: Use 8 threads (matches our 8 CPUs)\n",
    "    # -p: Show progress\n",
    "    # -O $OUT_DIR: Set the output directory\n",
    "    # -S: Split files (for paired-end, though ours is single-end)\n",
    "    fasterq-dump $SRR_ID -e 8 -p -O $OUT_DIR\n",
    "    \n",
    "    # Check if the download was successful\n",
    "    if [ $? -eq 0 ]; then\n",
    "        echo \"Successfully downloaded: $SRR_ID\"\n",
    "    else\n",
    "        echo \"ERROR: Failed to download $SRR_ID\"\n",
    "    fi\n",
    "    \n",
    "done < \"$ID_LIST_FILE\"\n",
    "\n",
    "echo \"----------------------------------------\"\n",
    "echo \"All downloads complete.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
