{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4276ba4-f551-4db0-bbd0-65be3966dfcf",
   "metadata": {},
   "source": [
    "# Notebook 02: Denoising with DADA2\n",
    "\n",
    "### Objective\n",
    "The goal of this notebook is to take our raw, paired-end FASTQ files and process them using the DADA2 pipeline. This will correct sequencing errors, merge our paired reads (R1 & R2), and remove chimeras.\n",
    "\n",
    "The final output will be our two most important files for downstream analysis:\n",
    "1.  **Feature Table (`table.qza`):** A matrix of Amplicon Sequence Variants (ASVs) by samples (i.e., the counts).\n",
    "2.  **Representative Sequences (`rep-seqs.qza`):** The unique DNA sequence for each ASV.\n",
    "\n",
    "### Workflow\n",
    "1.  **Create a Manifest File:** We need to create a \"map\" (`manifest.csv`) that tells QIIME 2 where to find the R1 and R2 files for each sample.\n",
    "2.  **Import Data:** We will use the manifest file to import our 255 samples into a single QIIME 2 artifact (`.qza` file).\n",
    "3.  **Run DADA2:** We will run `qiime dada2 denoise-paired` using the truncation parameters we found in Notebook 01 (`--p-trunc-len-f 160` and `--p-trunc-len-r 160`).\n",
    "4.  **Analyze DADA2 Stats:** We will inspect the summary statistics from DADA2 to see how many reads we successfully filtered, denoised, merged, and non-chimeric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0684133-8e62-4e54-884e-3dfb3e152de7",
   "metadata": {},
   "source": [
    "### 1. Create the Paired-End Manifest File\n",
    "\n",
    "To import our data into QIIME 2, we must first create a \"manifest file\". This is a CSV file that tells QIIME 2 three things for every sample:\n",
    "1.  `sample-id`: A unique name for the sample. We will use the `Run` ID (e.g., `SRR7013947`) as our unique sample ID.\n",
    "2.  `absolute-filepath-r1`: The full, absolute path to its R1 FASTQ file.\n",
    "3.  `absolute-filepath-r2`: The full, absolute path to its R2 FASTQ file.\n",
    "\n",
    "We will build this file using our `SraRunTable.csv` and Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4eb62f-85f5-45c5-9135-9cb9be7bd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os # We'll need this to get the absolute path\n",
    "\n",
    "# --- 1. Get the Absolute Path ---\n",
    "# Get the path of the *current* directory (which is 'notebooks/')\n",
    "# The !pwd command returns a list, so we take the first element [0]\n",
    "current_dir = !pwd\n",
    "        \n",
    "# Go one level up to get the main project directory path\n",
    "# This will be something like '/home/refm_youssef/16S_microbiome_analysis_crohns_disease'\n",
    "project_root = os.path.dirname(current_dir[0])\n",
    "\n",
    "print(f\"Project Root Path: {project_root}\")\n",
    "\n",
    "# --- 2. Load the Metadata ---\n",
    "metadata_file = \"../data/SraRunTable.csv\"\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "        \n",
    "print(f\"\\nLoaded {len(metadata_df)} records from SraRunTable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f452c-3bc7-46c5-9cad-09c583612ffb",
   "metadata": {},
   "source": [
    "### 2. Build the Clean and Docker-Ready Manifest File\n",
    "\n",
    "The standard manifest creation process failed previously due to strict technical requirements from the QIIME 2 version inside our Docker container. We must address three critical points in this step:\n",
    "\n",
    "1.  **Metadata Format:** QIIME 2 requires all metadata files, including the manifest, to be **Tab-Separated Values (TSV)**, which we ensure by saving the file with `.tsv` extension and setting `sep='\\t'`.\n",
    "2.  **Clean Data:** We filter out the known missing sample (`SRR7014200`) to prevent pipeline failure during file import.\n",
    "3.  **Docker Path Mapping:** The paths are defined relative to the Docker mount point (`/data`), specifically using `/data/data/raw_fastq/` as the correct path for the files inside the container environment.\n",
    "\n",
    "This guarantees a successful import into the QIIME 2 artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6085c5-11a8-4b16-824a-4506a4ebf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Create a new, empty DataFrame for our manifest\n",
    "manifest_df = pd.DataFrame()\n",
    "\n",
    "# 2. Filter out the known missing sample\n",
    "clean_metadata = metadata_df[metadata_df['Run'] != 'SRR7014200'].copy()\n",
    "print(f\"Filtered out 1 known missing sample. Proceeding with {len(clean_metadata)} samples.\")\n",
    "\n",
    "# 3. Create the 'sample-id' column\n",
    "manifest_df['sample-id'] = clean_metadata['Run']\n",
    "\n",
    "# 4. Create the 'forward-absolute-filepath' column (Correct Header + Docker Path)\n",
    "# Note: We use /data/data/ because we mount $(pwd) to /data\n",
    "manifest_df['forward-absolute-filepath'] = clean_metadata['Run'].apply(\n",
    "    lambda run_id: f\"/data/data/raw_fastq/{run_id}_1.fastq\"\n",
    ")\n",
    "\n",
    "# 5. Create the 'reverse-absolute-filepath' column (Correct Header + Docker Path)\n",
    "manifest_df['reverse-absolute-filepath'] = clean_metadata['Run'].apply(\n",
    "    lambda run_id: f\"/data/data/raw_fastq/{run_id}_2.fastq\"\n",
    ")\n",
    "\n",
    "# 6. Define the output path (using .tsv extension)\n",
    "manifest_output_file = \"../data/docker_manifest_clean.tsv\"\n",
    "\n",
    "# 7. Save to TSV (Tab Separated)\n",
    "# We use sep='\\t' to make it a TSV file\n",
    "manifest_df.to_csv(manifest_output_file, index=False, sep='\\t')\n",
    "\n",
    "# 8. Print the first 5 rows of our *new* manifest to verify\n",
    "print(f\"\\nClean, Docker-ready TSV manifest saved to: {manifest_output_file}\")\n",
    "manifest_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e1a41-59dc-4eae-87d6-d34cf2164ca1",
   "metadata": {},
   "source": [
    "### 3. The Import Problem: \"Docker Hang\"\n",
    "\n",
    "Our original plan was to import all 255 samples at once. However, during testing in the terminal, we discovered a major technical bottleneck:\n",
    "\n",
    "* **The Problem:** Running `qiime tools import` on all 255 samples (510 files) at once causes the Docker container to hang (freeze) indefinitely.\n",
    "* **The Solution:** The only robust workaround is to **split** our 255 samples into small \"batches\" (e.g., 10 samples per batch) and import them one by one.\n",
    "\n",
    "This \"Split-Apply-Combine\" strategy is the correct workflow for this dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3452d71-31d6-47ca-a018-6c9b7c9d0fc7",
   "metadata": {},
   "source": [
    "### 4. Step 1: Splitting the Manifest\n",
    "\n",
    "First, we will split our clean manifest (`docker_manifest_clean.tsv`) into 26 smaller batch files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84bbea-a7ff-4a3d-8259-37032b0f4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# --- Settings ---\n",
    "INPUT_MANIFEST = \"../data/docker_manifest_clean.tsv\"\n",
    "OUTPUT_DIR = \"../data/manifest_batches\"\n",
    "BATCH_SIZE = 10\n",
    "# ------------------\n",
    "\n",
    "# Ensure the output directory exists\n",
    "!mkdir -p {OUTPUT_DIR}\n",
    "\n",
    "# Read the clean manifest we just created\n",
    "clean_df = pd.read_csv(INPUT_MANIFEST, sep='\\t')\n",
    "num_samples = len(clean_df)\n",
    "num_batches = math.ceil(num_samples / BATCH_SIZE)\n",
    "\n",
    "print(f\"Splitting {num_samples} samples into {num_batches} batches of {BATCH_SIZE}...\")\n",
    "\n",
    "for i in range(num_batches):\n",
    "    # Calculate the start and end index for this batch\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = (i + 1) * BATCH_SIZE\n",
    "    \n",
    "    # Slice the DataFrame\n",
    "    batch_df = clean_df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Define the output filename (e.g., batch_01.tsv)\n",
    "    batch_num = i + 1\n",
    "    batch_filename = f\"{OUTPUT_DIR}/batch_{batch_num:02d}.tsv\"\n",
    "    \n",
    "    # Save this batch as a TSV\n",
    "    batch_df.to_csv(batch_filename, index=False, sep='\\t')\n",
    "\n",
    "print(f\"\\nSuccessfully created {num_batches} manifest files in {OUTPUT_DIR}\")\n",
    "\n",
    "# Verify by listing the files\n",
    "!ls -lh {OUTPUT_DIR} | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a0ecf-ed14-4490-83a2-da5eebacf46f",
   "metadata": {},
   "source": [
    "### 5. Step 2: DADA2 Denoising (Batch Processing)\n",
    "\n",
    "Now that our data is split, the correct workflow is:\n",
    "1.  **Import** a batch.\n",
    "2.  **Run DADA2** on that imported batch.\n",
    "3.  Repeat for all batches.\n",
    "4.  Merge the final results.\n",
    "\n",
    "Let's start by testing this full pipeline on **Batch 01** only.\n",
    "\n",
    "**(Warning: This next cell will take some time, as it's running the full DADA2 pipeline on 10 samples.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285807e-15e4-4359-b70f-5b17f9890fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# --- Settings ---\n",
    "BATCH_NUM = \"01\" # We are testing Batch 01 first\n",
    "TRUNC_LEN_F = 160\n",
    "TRUNC_LEN_R = 160\n",
    "\n",
    "# --- Directories (relative to the notebook location) ---\n",
    "MANIFEST_DIR = \"../data/manifest_batches\"\n",
    "IMPORT_DIR = \"../results/03_qiime_artifacts\"\n",
    "TABLES_DIR = \"../results/04_dada2_tables\"\n",
    "REP_SEQS_DIR = \"../results/05_dada2_rep-seqs\"\n",
    "STATS_DIR = \"../results/06_dada2_stats\"\n",
    "\n",
    "# --- Paths for this specific batch ---\n",
    "manifest_file = f\"{MANIFEST_DIR}/batch_{BATCH_NUM}.tsv\"\n",
    "imported_qza = f\"{IMPORT_DIR}/batch_{BATCH_NUM}.qza\"\n",
    "table_qza = f\"{TABLES_DIR}/table-batch_{BATCH_NUM}.qza\"\n",
    "rep_seqs_qza = f\"{REP_SEQS_DIR}/rep-seqs-batch_{BATCH_NUM}.qza\"\n",
    "stats_qza = f\"{STATS_DIR}/stats-batch_{BATCH_NUM}.qza\"\n",
    "\n",
    "# --- Create Output Directories ---\n",
    "# Use -p to avoid errors if they already exist\n",
    "!mkdir -p {IMPORT_DIR} {TABLES_DIR} {REP_SEQS_DIR} {STATS_DIR}\n",
    "\n",
    "print(f\"--- Starting Test Pipeline for Batch {BATCH_NUM} ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Step 1/2: Import this batch ---\n",
    "print(f\"Step 1/2: Importing {manifest_file}...\")\n",
    "\n",
    "# We mount the project root (../) to /data\n",
    "# We set the working directory to /data/notebooks\n",
    "!docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "  qiime2/core:latest \\\n",
    "  qiime tools import \\\n",
    "    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n",
    "    --input-path {manifest_file} \\\n",
    "    --output-path {imported_qza} \\\n",
    "    --input-format PairedEndFastqManifestPhred33V2\n",
    "\n",
    "# --- Verification checkpoint ---\n",
    "# Check if the import *actually* created the file\n",
    "# We check the path relative to the notebook (../results/...)\n",
    "if not os.path.exists(f\"../{imported_qza}\"):\n",
    "    print(f\"!!! ERROR: Import failed for Batch {BATCH_NUM}. Stopping.\")\n",
    "else:\n",
    "    print(f\"Import successful: {imported_qza}\")\n",
    "    \n",
    "    # --- Step 2/2: Run DADA2 on this batch ---\n",
    "    print(f\"\\nStep 2/2: Running DADA2 on {imported_qza}...\")\n",
    "    !docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "      qiime2/core:latest \\\n",
    "      qiime dada2 denoise-paired \\\n",
    "        --i-demultiplexed-seqs {imported_qza} \\\n",
    "        --p-trunc-len-f {TRUNC_LEN_F} \\\n",
    "        --p-trunc-len-r {TRUNC_LEN_R} \\\n",
    "        --o-table {table_qza} \\\n",
    "        --o-representative-sequences {rep_seqs_qza} \\\n",
    "        --o-denoising-stats {stats_qza} \\\n",
    "        --p-n-threads 0 # Use all available threads\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"--- Test Pipeline for Batch {BATCH_NUM} finished in {(end_time - start_time):.2f} seconds. ---\")\n",
    "\n",
    "    # --- Final Verification ---\n",
    "    print(\"\\nVerifying DADA2 outputs:\")\n",
    "    !ls -lh ../{table_qza}\n",
    "    !ls -lh ../{rep_seqs_qza}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d79b32-767e-4ba5-bf99-42b808cb0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  Run DADA2 on the Imported Batch 01 ---\n",
    "# The previous cell successfully imported batch_01.qza.\n",
    "# Now we run Step 2/2 (DADA2) on that artifact.\n",
    "\n",
    "print(f\"\\nStep 2/2: Running DADA2 on {imported_qza}...\")\n",
    "print(\"(This is the slow step, please wait...)\")\n",
    "\n",
    "# Start timer for DADA2 step\n",
    "dada2_start_time = time.time()\n",
    "\n",
    "# --- Run DADA2 Command ---\n",
    "# (Note: We use the *exact same* variables from the cell above)\n",
    "!docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "  qiime2/core:latest \\\n",
    "  qiime dada2 denoise-paired \\\n",
    "    --i-demultiplexed-seqs {imported_qza} \\\n",
    "    --p-trunc-len-f {TRUNC_LEN_F} \\\n",
    "    --p-trunc-len-r {TRUNC_LEN_R} \\\n",
    "    --o-table {table_qza} \\\n",
    "    --o-representative-sequences {rep_seqs_qza} \\\n",
    "    --o-denoising-stats {stats_qza} \\\n",
    "    --p-n-threads 0 # Use all available threads\n",
    "\n",
    "dada2_end_time = time.time()\n",
    "\n",
    "# --- Final Verification (Corrected) ---\n",
    "# We check the paths *without* the extra \"../\"\n",
    "if os.path.exists(table_qza) and os.path.exists(rep_seqs_qza):\n",
    "    print(f\"\\n--- DADA2 Pipeline for Batch {BATCH_NUM} finished in {(dada2_end_time - dada2_start_time):.2f} seconds. ---\")\n",
    "    print(\"\\nVerifying DADA2 outputs:\")\n",
    "    !ls -lh {table_qza}\n",
    "    !ls -lh {rep_seqs_qza}\n",
    "else:\n",
    "    print(f\"\\n!!! ERROR: DADA2 failed. Output files not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfbb578-332e-475e-a709-75175d647475",
   "metadata": {},
   "source": [
    "### 6. Run the Full Pipeline on All Batches (02-26)\n",
    "\n",
    "Our test on Batch 01 was a complete success. We have confirmed that the two-step pipeline (Import, then DADA2) works perfectly.\n",
    "\n",
    "Now, we will create a `for` loop to apply this exact same pipeline to the remaining 25 batches (from `batch_02.tsv` to `batch_26.tsv`).\n",
    "\n",
    "**(Warning: This next cell will take a long time to complete!)**\n",
    "Based on our test (3.6 minutes for 1 batch), we expect this loop to take:\n",
    "`25 batches * 3.6 minutes/batch â‰ˆ 90 minutes (1.5 hours)`\n",
    "\n",
    "We will run this cell and let it process all remaining samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de522b15-d8a5-4434-962a-37128209a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  Process Batches 02 through 26 ---\n",
    "print(f\"--- Starting Full Pipeline for Batches 02-26 ---\")\n",
    "\n",
    "# We loop from 2 up to and including 26\n",
    "for i in range(2, 27):\n",
    "    # --- Settings for this batch ---\n",
    "    BATCH_NUM = f\"{i:02d}\" # (02, 03, 04, ...)\n",
    "    \n",
    "    # Define all paths\n",
    "    manifest_file = f\"{MANIFEST_DIR}/batch_{BATCH_NUM}.tsv\"\n",
    "    imported_qza = f\"{IMPORT_DIR}/batch_{BATCH_NUM}.qza\" \n",
    "    table_qza = f\"{TABLES_DIR}/table-batch_{BATCH_NUM}.qza\"\n",
    "    rep_seqs_qza = f\"{REP_SEQS_DIR}/rep-seqs-batch_{BATCH_NUM}.qza\"\n",
    "    stats_qza = f\"{STATS_DIR}/stats-batch_{BATCH_NUM}.qza\"\n",
    "    \n",
    "    print(f\"\\n--- Processing Batch {BATCH_NUM} ---\")\n",
    "    batch_start_time = time.time()\n",
    "\n",
    "    # --- Step 1/2: Import this batch ---\n",
    "    print(f\"Step 1/2: Importing {manifest_file}...\")\n",
    "    !docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "      qiime2/core:latest \\\n",
    "      qiime tools import \\\n",
    "        --type 'SampleData[PairedEndSequencesWithQuality]' \\\n",
    "        --input-path {manifest_file} \\\n",
    "        --output-path {imported_qza} \\\n",
    "        --input-format PairedEndFastqManifestPhred33V2\n",
    "\n",
    "    # --- Verification checkpoint (Corrected) ---\n",
    "    # We check the *correct* path: imported_qza (which is '../results/...')\n",
    "    if not os.path.exists(imported_qza):\n",
    "        print(f\"!!! ERROR: Import failed for Batch {BATCH_NUM}. Stopping loop.\")\n",
    "        # Stop the whole loop if one batch fails\n",
    "        break \n",
    "    else:\n",
    "        print(f\"Import successful: {imported_qza}\")\n",
    "        \n",
    "        # --- Step 2/2: Run DADA2 on this batch ---\n",
    "        print(f\"\\nStep 2/2: Running DADA2 on {imported_qza}...\")\n",
    "        !docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "          qiime2/core:latest \\\n",
    "          qiime dada2 denoise-paired \\\n",
    "            --i-demultiplexed-seqs {imported_qza} \\\n",
    "            --p-trunc-len-f {TRUNC_LEN_F} \\\n",
    "            --p-trunc-len-r {TRUNC_LEN_R} \\\n",
    "            --o-table {table_qza} \\\n",
    "            --o-representative-sequences {rep_seqs_qza} \\\n",
    "            --o-denoising-stats {stats_qza} \\\n",
    "            --p-n-threads 0 # Use all available threads\n",
    "        \n",
    "        batch_end_time = time.time()\n",
    "        \n",
    "        # Final check for DADA2 outputs\n",
    "        if os.path.exists(table_qza):\n",
    "            print(f\"--- Batch {BATCH_NUM} finished successfully in {(batch_end_time - batch_start_time):.2f} seconds. ---\")\n",
    "        else:\n",
    "            print(f\"!!! ERROR: DADA2 failed for Batch {BATCH_NUM}. Stopping loop.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n--- Full Batch Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95813e-41bb-490f-aad3-6ee52ae1d9ce",
   "metadata": {},
   "source": [
    "### 7. Merge All DADA2 Results\n",
    "\n",
    "The batch processing (Cell 14) was a complete success. We now have 26 individual Feature Tables (`table-batch_XX.qza`) and 26 individual Representative Sequence files (`rep-seqs-batch_XX.qza`).\n",
    "\n",
    "The final step in this stage is to merge them into two final, complete artifacts. We will do this in two steps:\n",
    "1.  Merge all 26 feature tables.\n",
    "2.  Merge all 26 representative sequence files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82d594-70d6-4460-9324-da1b582b809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Cell 17) Merge all 26 Feature Tables (Corrected) ---\n",
    "\n",
    "print(\"--- 1. Merging all 26 Feature Tables ---\")\n",
    "# The previous command failed because --i-tables needs a *list* of files, not a directory.\n",
    "# We will use a wildcard (*) to pass all 26 files at once.\n",
    "\n",
    "!docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "  qiime2/core:latest \\\n",
    "  qiime feature-table merge \\\n",
    "    --i-tables ../results/04_dada2_tables/table-batch_*.qza \\\n",
    "    --o-merged-table ../results/table.qza\n",
    "\n",
    "print(\"\\n--- 2. Verification of Final Merged Table ---\")\n",
    "# Check if the final merged file exists\n",
    "!ls -lh ../results/table.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f78c7-8cc1-44ea-9f01-dce6ba6f02f5",
   "metadata": {},
   "source": [
    "### 8. Merge Representative Sequences\n",
    "\n",
    "We have successfully merged our 26 feature tables into `table.qza`. Now, we must do the same for the representative sequences.\n",
    "\n",
    "This will combine all 26 `rep-seqs-batch_XX.qza` files (from the `results/05_dada2_rep-seqs/` directory) into a single, final `rep-seqs.qza` artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701ab7b-d17d-4367-b064-ba9030d41bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  Merge all 26 Rep-Seqs (Corrected) ---\n",
    "\n",
    "print(\"--- 1. Merging all 26 Rep-Seqs files (Corrected Command) ---\")\n",
    "# The previous command failed due to old parameter names in QIIME 2 v2020.8\n",
    "# We are using --i-data (instead of --i-sequences)\n",
    "# We are using --o-merged-data (instead of --o-merged-sequences)\n",
    "\n",
    "!docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "  qiime2/core:latest \\\n",
    "  qiime feature-table merge-seqs \\\n",
    "    --i-data ../results/05_dada2_rep-seqs/rep-seqs-batch_*.qza \\\n",
    "    --o-merged-data ../results/rep-seqs.qza\n",
    "\n",
    "print(\"\\n--- 2. Verification of Final Merged Rep-Seqs ---\")\n",
    "# Check if the final merged file exists\n",
    "!ls -lh ../results/rep-seqs.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94ce67-0ede-4f10-aa6c-adf3d135ab66",
   "metadata": {},
   "source": [
    "### 9. Summarize Final Artifacts\n",
    "\n",
    "We have successfully completed the DADA2 pipeline and merged all results. We now have our two final, critical artifacts:\n",
    "1.  `table.qza` (The final Feature Table)\n",
    "2.  `rep-seqs.qza` (The final Representative Sequences)\n",
    "\n",
    "As a final step in this notebook, we will summarize both of these artifacts to get an overview of our dataset. This will also generate visual `.qzv` files that we can view in `qiime2 view`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22c46b-9ef7-434f-9fe3-d9efa89815b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---  Create a QIIME 2-compatible Metadata File ---\n",
    "\n",
    "print(\"--- 1. Loading original SraRunTable.csv ---\")\n",
    "metadata_df = pd.read_csv(\"../data/SraRunTable.csv\")\n",
    "\n",
    "# --- 2. Renaming 'Run' column to 'sample-id' ---\n",
    "# This is the ID column QIIME 2 expects.\n",
    "# We use 'inplace=True' to modify the DataFrame directly.\n",
    "metadata_df.rename(columns={'Run': 'sample-id'}, inplace=True)\n",
    "\n",
    "# --- 3. Saving as a Tab-Separated (TSV) file ---\n",
    "METADATA_TSV_PATH = \"../data/metadata.tsv\"\n",
    "metadata_df.to_csv(METADATA_TSV_PATH, sep='\\t', index=False)\n",
    "\n",
    "print(f\"\\n--- 4. Successfully created clean metadata file: {METADATA_TSV_PATH} ---\")\n",
    "\n",
    "print(\"\\n--- 5. Verification (showing first 2 lines of the new TSV file) ---\")\n",
    "!head -n 2 {METADATA_TSV_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9bed2-2630-4ccf-a281-4898f868341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  Summarize the Final Feature Table (Corrected) ---\n",
    "\n",
    "print(\"--- 1. Summarizing the merged Feature Table (using metadata.tsv) ---\")\n",
    "\n",
    "!docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "  qiime2/core:latest \\\n",
    "  qiime feature-table summarize \\\n",
    "    --i-table ../results/table.qza \\\n",
    "    --o-visualization ../results/table.qzv \\\n",
    "    --m-sample-metadata-file ../data/metadata.tsv\n",
    "\n",
    "print(\"\\n--- 2. Verification of Table Summary ---\")\n",
    "!ls -lh ../results/table.qzv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45b7cf-4431-448f-94ba-a194addf1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  Summarize the Final Rep-Seqs (Corrected Plugin Name) ---\n",
    "\n",
    "print(\"--- 1. Summarizing the merged Rep-Seqs (Corrected Command) ---\")\n",
    "# The previous command failed due to old plugin names in QIIME 2 v2020.8\n",
    "# We are using 'qiime feature-table tabulate-seqs'\n",
    "# (instead of 'qiime feature-data ...')\n",
    "\n",
    "!docker run --rm -v $(pwd)/..:/data -w /data/notebooks \\\n",
    "  qiime2/core:latest \\\n",
    "  qiime feature-table tabulate-seqs \\\n",
    "    --i-data ../results/rep-seqs.qza \\\n",
    "    --o-visualization ../results/rep-seqs.qzv\n",
    "\n",
    "print(\"\\n--- 2. Verification of Rep-Seqs Summary ---\")\n",
    "# Check if the final merged file exists\n",
    "!ls -lh ../results/rep-seqs.qzv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ed62f-f6b2-48a8-989c-c05945285845",
   "metadata": {},
   "source": [
    "### 11. Conclusion & Next Steps\n",
    "\n",
    "This notebook was the most computationally intensive part of our pipeline. We successfully addressed the major \"Docker Hang\" bottleneck by implementing a \"Split-Apply-Combine\" strategy, which is a robust method for handling large datasets.\n",
    "\n",
    "We have successfully:\n",
    "1.  Split our 255 samples into 26 manageable batches.\n",
    "2.  Run the full DADA2 pipeline (Import + Denoise) on all 26 batches.\n",
    "3.  Merged the 26 individual results into two final, project-wide artifacts.\n",
    "4.  Created a clean, QIIME 2-compatible metadata file (`metadata.tsv`).\n",
    "5.  Summarized and validated all final outputs.\n",
    "\n",
    "**Final Validated Artifacts:**\n",
    "* `results/table.qza`: The final ASV Feature Table (Samples x Features).\n",
    "* `results/rep-seqs.qza`: The final ASV Representative Sequences (Feature ID -> DNA Sequence).\n",
    "* `results/table.qzv`: A visual summary of the feature table and sample depths.\n",
    "* `results/rep-seqs.qzv`: A visual summary of the sequence data.\n",
    "\n",
    "**Next Steps:**\n",
    "With our clean, validated data ready, we can now move on to **Notebook 03**. In the next stage, we will finally answer the \"what\" question:\n",
    "1.  We will analyze the `table.qzv` summary to understand our final read counts and sample quality.\n",
    "2.  We will perform **Taxonomic Classification** on our `rep-seqs.qza` to assign bacterial names (like *E. coli* or *Lactobacillus*) to our ASVs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
